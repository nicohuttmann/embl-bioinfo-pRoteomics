[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Hi friends, welcome Introduction Data Analysis Visualization R. document contains explanations material group project produced RStudio using R Markdown document ‘knitted’ html file. ’ll maybe get later.1This book live Github repository updated course. end, can download final project alongside additional material like PowerPoint presentation guide us sections, code, raw data maybe stuff. Everything see course available afterwards.","code":""},{"path":"index.html","id":"group-work-structure","chapter":"Preface","heading":"Group work structure","text":"can go project individually groups. case, recommend putting code R script matter . step, problem facing described alongside final goal. , can either figure code entirely , use hint, copy complete code snippet document run . latter option comes close many problems coding solved - define problem, google , find answer StackOverflow copy script. Yay, ’re becoming data scientist!first chapters document R Basics (left) provide short introduction coding R. Parts reference book R Data Science (2e). great resource everyone beginner expert. recommend matter , relatively new R course, go days use opportunity discuss concepts book group setting. first 2.5 h session Tuesday suffice get everyone page ready start exploring real data Group Work Project.game plan now :Tuesday\n12:30 - 15:00 R Basics\n15:30 - 17:00 Getting started projectWednesday\n16:30 - 17:30 Pick project see everything still works2Thursdays\n9:00 - 12:00 Finish data analysis\n13:00 - 16:00 Try data visualizations reportingFriday\n9:00 - 11:30 Prepare presentation breakfast","code":""},{"path":"index.html","id":"motivation","chapter":"Preface","heading":"Motivation","text":"Breakfast Friday, isn’t motivation enough? Well, . ’re spend 13.5 h project together precious research waiting us.one’s required use R project, believe already plenty motivation. course give opportunity see actual R code3, just random examples. may even use code remotely interested anything regarding protein-protein, protein-nucleic acid, protein-metabolite, protein-drug interactions proteome-wide scale.one’s need R projects yet, see fast automation excel tasks. GraphPad Prism. maybe want make simple website. wanna put CV.4We can condensed way.","code":""},{"path":"r-and-rstudio.html","id":"r-and-rstudio","chapter":"1 R and RStudio","heading":"1 R and RStudio","text":"“R ‘GNU S’, freely available language environment statistical computing graphics provides wide variety statistical graphical techniques: linear nonlinear modelling, statistical tests, time series analysis, classification, clustering, etc.”description R R website.","code":""},{"path":"r-and-rstudio.html","id":"install-the-r-language","chapter":"1 R and RStudio","heading":"1.1 Install the R language","text":"Download recent release R platform https://cloud.r-project.org/. Install like program. important install R RStudio.","code":""},{"path":"r-and-rstudio.html","id":"install-rstudio","chapter":"1 R and RStudio","heading":"1.2 Install RStudio","text":"RStudio integrated development environment (IDE) R. IDEs exist, RStudio software choice far. Like, far. Download desktop version RStudio https://www.rstudio.com/products/rstudio/. program interact R language conduct analyses.","code":""},{"path":"how-does-r-work.html","id":"how-does-r-work","chapter":"2 How does R work?","heading":"2 How does R work?","text":"“understand computations R, two slogans helpful:\nEverything exists object.\nEverything happens function call.” — John Chambers5Ok, cool. now? Well, statement tells us everything R code falls two categories, either object function (likely take object input give back modified object return).","code":""},{"path":"how-does-r-work.html","id":"objects","chapter":"2 How does R work?","heading":"2.1 Objects","text":"Please look Workflow: basics first introduction recapitulation6 coding R.\nimportant takeaway can assign/create object R assignment operator <-. , c ombine function c()7 one frequently used functions R.can simply copy code marking pressing helpful button top right corner code chunk paste R script. just run lines code look produce.c(1, 2, 3) -> numbers works well, please ever .8\ncreated called vector, precisely atomic vector. means elements type. can read Vectors chapter Advanced R waiting time unsure behavior vectors. Examples :unhappy type data changed previous example, simply change c() list().9\ndata types can found Vectors chapter mentioned previously covered throughout project.","code":"\n# Assign the vector to an object called numbers\nnumbers <- c(1, 2, 3)\n# Execute the following line to print it to the console\nnumbers\n# Some objects (not this one) need to be forced to show in the console \nprint(numbers)\nmixed_numbers <- c(0, 1, 2, \"three\")\nmixed_numbers_2 <- c(F, TRUE, 2, 3)\n# You don't actually need to assign them\nc(F, TRUE, 2, \"three\")\nlist(0, 1, 2, \"three\")\nlist(F, TRUE, 2, 3)\nlist(\"these\" = F, \"are\" = TRUE, \"names\" = 2, \"three\")\nlist(\"and\" = F, \n     \"they\" = TRUE, \n     \"work\" = 2, \n     \"notemptyanymore\" = \"for both lists and vectors :)\")"},{"path":"how-does-r-work.html","id":"functions","chapter":"2 How does R work?","heading":"2.2 Functions","text":"Besides basic object R, vector, also covered two functions well. c() list() functions allow us create atomic vector list objects. want understand function, simply write ?c10 R prompt documentation function Help pane bottom right.11The number functions exceeds number object types far, therefore see use functions properly. function 1. name, 2. object 3. can evoked () brakets following name. Let’s try :Nice! bad example. Write function name sum wait list options appear. happen, can press Tab. see list, scroll arrow buttons choose function Tab Enter. cursor brackets function, press Tab . Now, see arguments function. Arguments different inputs function declare precisely input goes within function. bad example special type argument ..., (dot-dot-dot), accepts undefined number arguments, case multiple numbers.better example intersect() function, takes two sets form atomic vectors x y returns intersection.Set Operation functions useful one point sure. Another resource worth mentioning example generalis R-bloggers website.formal description functions, can read Function fundamentals. now let’s see functions come .","code":"\nsum(1, 2, 3)\nintersect(x = c(1, 2, 3), \n          y = c(2, 3, 4))"},{"path":"r-packages.html","id":"r-packages","chapter":"3 R packages","heading":"3 R packages","text":"“R packages collections functions data sets developed community. increase power R improving existing base R functionalities, adding new ones.”12Basically, R packages nothing collections functions bundled together way makes sense. Like different cookbooks contain recipes particular kind food. can installed many different sources explored .","code":""},{"path":"r-packages.html","id":"from-cran","chapter":"3 R packages","heading":"3.1 From CRAN","text":"Comprehensive R Archive Network (CRAN) package repository features 18,000+ R packages. ’s list Available CRAN Packages Name. general purpose packages can found , however due reasons, packages available sources.first example, download tidyverse, collection R packages data science.“tidyverse opinionated collection R packages designed data science. packages share underlying design philosophy, grammar, data structures.”can install packages CRAN install.packages() function like :Note, can also download one package using vector (c()) containing package names:BiocManager well devtools used following download R packages sources.","code":"\ninstall.packages(\"tidyverse\")\ninstall.packages(c(\"BiocManager\", \"devtools\"))"},{"path":"r-packages.html","id":"from-bioconductor","chapter":"3 R packages","heading":"3.2 From Bioconductor","text":"Bioconductor collection R packages bioinformatics purposes. first packages need Bioconductor downloaded install() function BiocManager package:","code":"\nBiocManager::install(c(\"fgsea\",\n                       \"org.Hs.eg.db\",\n                       \"UniProt.ws\"))"},{"path":"r-packages.html","id":"from-github-and-others-sources","chapter":"3 R packages","heading":"3.3 From GitHub and others sources","text":"Another important source R packages GitHub. GitHub just place R packages developed put repositories CRAN Bioconductor, many packages including PELSA package can installed well.example install_github() function devtools package. package required yet.","code":"\ndevtools::install_github(\"nicohuttmann/PELSA\")"},{"path":"the-tidyverse.html","id":"the-tidyverse","chapter":"4 The tidyverse","heading":"4 The tidyverse","text":"may already installed tidyverse previous chapter. collection many R packages made data science. see high level dialect - allows base R“tidyverse opinionated collection R packages designed data science. packages share underlying design philosophy, grammar, data structures.”tidyverse extensively described R Data Science (2e) book13 recommended Learn tidyverse individual packages summarized Posit cheatsheets.","code":""},{"path":"coding-style.html","id":"coding-style","chapter":"5 Coding style","heading":"5 Coding style","text":"Okay, nearly made project. One last chapter. promise.One decision may already guessed, follower tidyverse try use instead base R solutions wherever possible.14 Still, almighty sometimes need solutions. example dividing two data frames require tidyverse functions pivot_longer, inner_join, mutate pivot_wider. elegant base R solution data matrices simply using mathematical operator /.Damn, writing example, found trying show work, somehow work. let explain don’t like example.structure object R can found using class() function. Just try .objects base R example matrices. Let’s check tidyverse example.output divide operation tibble/“tbl_df” object anymore, data.frame. big problem now, can solved quite easily tibble::as_tibble() function:However, recommended change class data way analysis.real tidyverse solution work following. key combine tibbles one allows us divide columns simple dplyr::mutate() operation.seems like tedious example comparison, also bears ’s advantages see later.Btw, ’ve done code-condensed form.need understand immediately, common operations omics data science. understand tidyr::pivot_longer/wider functions within week, ’re years ahead compared journey data science :)","code":"\nm1 <-  matrix(1:9, nrow = 3)\n\nm2 <-  matrix(1:9, nrow = 3) * 2\n\nm2 / m1\n# Making the tibble works slightly different in our exampe \nt1 <- tibble(a = 1:3, b = 4:6, c = 7:9)\n# and multiplying each element by 2 gets done with mutate and across. \n# We will get back to this later\nt2 <- tibble(a = 1:3, b = 4:6, c = 7:9) %>% \n  # \\(x) x * 2 is equivalent to function(x) x * 2\n  mutate(across(everything(), \\(x) x * 2))\n\nt2 / t1\nclass(m1)\n\nclass(m2)\n\nclass(m2 / m1)\nclass(t1)\n\nclass(t2)\n\nclass(t2 / t1)\ntibble::as_tibble(t2 / t1)\n# First we add a row column to the tibbles, this brings them closer to real data \nt1_r <- t1 %>% \n  mutate(row = c(\"1\", \"2\", \"3\"), \n         .before = 1)\n# Do it again \nt2_r <- t2 %>% \n  mutate(row = c(\"1\", \"2\", \"3\"), \n         .before = 1)\n# Now we bring our data from a wide format into a long format \nt1_long <- pivot_longer(t1_r, \n                        cols = c(\"a\", \"b\", \"c\"), \n                        names_to = \"column\", \n                        values_to = \"number\")\n# And again \nt2_long <- pivot_longer(t2_r, \n                        cols = -1, \n                        names_to = \"column\", \n                        values_to = \"number\")\n\n# We start with combining the data frames by matching both tibbles by the \n# columns \"row\" and \"column\" and choosing a suffing for overlapping column names\nfull_join(t1_long, t2_long, by = c(\"row\", \n                                   \"column\"), \n          suffix = c(\"_1\", \"_2\")) %>% \n  # Then we can divide the values of t2 by t1 and save them in a new column \n  mutate(number = number_2 / number_1) %>% \n  # Now we finally pivot back to the wide data format from the beginning\n  pivot_wider(id_cols = \"row\", \n              names_from = \"column\", \n              values_from = \"number\")\n# First we put the tibbles in a list \nlist(\"t1\" = t1, \"t2\" = t2) %>% \n  # map allows us to do the same computation for all objects of the list as \n  # defined by the function \\(x) x...\n  map(\\(x) x %>% \n        mutate(row = c(\"1\", \"2\", \"3\"), \n               .before = 1) %>% \n        pivot_longer(cols = -1, \n                     names_to = \"column\", \n                     values_to = \"number\")) %>% \n  # Once both tibbles are ready to be combined, we can access them via the with \n  # function (we may talk about this again, I find it very helpful sometimes)\n  with(full_join(t1, t2, by = c(\"row\", \n                                \"column\"), \n                 suffix = c(\"_1\", \"_2\"))) %>% \n  # Now we continue as above\n  mutate(number = number_2 / number_1) %>% \n  # And back to a wide format tibble \n  pivot_wider(id_cols = \"row\", \n              names_from = \"column\", \n              values_from = \"number\")"},{"path":"coding-style.html","id":"coding-style-1","chapter":"5 Coding style","heading":"5.1 Coding style","text":"Well, ?! pipe. %>% . ?15We previously learned “Everything happens function call.” order result function ‘exist’, need assign object. Fine. Works. happens need multiple operations object get result?Let’s consider generic example vector contains letters like count represent barplot.first exercise book16. done code, observe happened continue code .code yields barplot represents frequency letter. get result, created two intermediate objects random_letters_table random_letters_table_sorted. can random_letters object, simply overwriting line.saw examples different ways chain outputs different computations arrive barplot. Let’s try use %>% operator simplify process.chain begins object random_letters followed functions subsequent line. output preceding line always used first argument following function.equivalent nested construct:17I hope obvious %>% useful tool, just writing less code also making code legible. Btw, find hidden message barplot? , try stretch ‘Plots’ window press ‘Zoom’ ‘Plots’ panel.making exercise even better example utility %>% :)\nprobably better explanation pipes -depth explanation, please look corresponding chapter R Data Science chapter second edition, R Data Science (2e).Using %>% pipe operator one part Style guide big impact legibility code. visually prefer old version suggest reading one two. beginning coding career, recommendations immediately obvious, remember later .","code":"\nletters_freq <- c(\"A\" = 5, \"B\" = 18, \"C\" = 20, \"D\" = 2,\n                  \"E\" = 24, \"F\" = 13, \"G\" = 1, \"H\" = 25,\n                  \"I\" = 21, \"J\" = 11, \"K\" = 19, \"L\" = 6,\n                  \"M\" = 10, \"N\" = 14, \"O\" = 16, \"P\" = 9,\n                  \"Q\" = 23, \"R\" = 17, \"S\" = 8, \"T\" = 26,\n                  \"U\" = 22, \"V\" = 7, \"W\" = 15, \"X\" = 12,\n                  \"Y\" = 3, \"Z\" = 4)\n\nrandom_letters <- rep(names(letters_freq), letters_freq)\n# Count each element of random_letters \nrandom_letters_table <- table(random_letters)\n# Sort table in decending order \nrandom_letters_table_sorted <- sort(random_letters_table, decreasing = T)\n# Plot the frequency of each letter \nbarplot(random_letters_table_sorted)\n# Count each element of random_letters \nrandom_letters <- table(random_letters)\n# Sort table in descending order \nrandom_letters <- sort(random_letters, decreasing = T)\n# Plot the frequency of each letter \nbarplot(random_letters)\n# We reassign random_letters since we overwrote it in the previous example\nrandom_letters <- rep(names(letters_freq), letters_freq)\n\nrandom_letters %>%  \n  table() %>% \n  sort(decreasing = T) %>% \n  barplot()\nbarplot(sort(table(random_letters), decreasing = T))\nsentence_full <- \"The quick brown fox jumps over the lazy dog\"\n## Unrelated but, wow I just discovered there's a `sentences` object in R\n\n# Generate the frequency of each individual letter\nletters_freq <- sentence_full %>% \n  # Remove spaces\n  str_remove_all(\" \") %>% \n  # Split the string into individual letters \n  str_split(\"\") %>% \n  # Extract vector from list \n  unlist() %>% \n  # Make all letters upper case\n  toupper() %>% \n  # Remove dublicated letters \n  unique() %>% \n  # Make vector with letters as names and numbers from 26 to 1 as content\n  # Do you see how the . marks the position of the argument coming from the previous line?\n  setNames(26:1, .) %>% \n  # Reorder the vector so that the solution is not immediately obvious to you guys \n  # Not this is some weird stuff here, do try this at home\n  `[`(., order(names(.)))\n\n# Test if it works\nrandom_letters <- rep(names(letters_freq), letters_freq)\n\n\n# Now prepare the example for the exercise\n\n# This function copies a vector to the console \n.cat_character_named <- function(...) {\n  \n  n <- paste0(names(...), '\" = \"', ..., '\"')\n  \n  cat(paste0('c(\"', paste(n, collapse = ',\\n\\t\"'), ')'))\n  \n}\n\n.cat_character_named(random_letters)\n\nletters_freq <- c(\"A\" = 5,\n                  \"B\" = 18,\n                  \"C\" = 20,\n                  \"D\" = 2,\n                  \"E\" = 24,\n                  \"F\" = 13,\n                  \"G\" = 1,\n                  \"H\" = 25,\n                  \"I\" = 21,\n                  \"J\" = 11,\n                  \"K\" = 19,\n                  \"L\" = 6,\n                  \"M\" = 10,\n                  \"N\" = 14,\n                  \"O\" = 16,\n                  \"P\" = 9,\n                  \"Q\" = 23,\n                  \"R\" = 17,\n                  \"S\" = 8,\n                  \"T\" = 26,\n                  \"U\" = 22,\n                  \"V\" = 7,\n                  \"W\" = 15,\n                  \"X\" = 12,\n                  \"Y\" = 3,\n                  \"Z\" = 4)"},{"path":"coding-style.html","id":"data-organization-wihtin-r","chapter":"5 Coding style","heading":"5.2 Data organization wihtin R","text":"moving basics actual project, let’s see work R. Working R means packages code use, working R means put code data. Let’s start top level. scripts data live?","code":""},{"path":"coding-style.html","id":"r-projects","chapter":"5 Coding style","heading":"5.2.1 R projects","text":"just work R project, also work one. Please look description R projects now well known book. get closer Group Work Project let’s start creating place data code.Create R project course. suggest make folder called R within Documents folder wherever store research. , start collect different R projects.\ncan create new R project clicking File -> New Project… -> New Directory -> New Project. Now enter name18, browse top folder like press Create Project. want keep current R session open, tick Open new session.\nmay seem convenient folder server lab, gets archived automatically, however, sometimes gives trouble slows analysis. Copying R folder time time seems like practical solution .\nGreat, created first R project. can identify project name window written top right corner.creating first R script, can set folder structure within R project. can done operating system’s file explorer function dir.create. usually begin setting work directory like :like :)","code":"\n# All R and Rmd scripts will be in here \ndir.create(\"Scripts\")\n# All raw data can be stored here \ndir.create(\"Data\")\n# A separate folder for RData objects \ndir.create(\"Data/RData\")\n# All output like figures, tables etc. \ndir.create(\"Output\")"},{"path":"coding-style.html","id":"lists-and-.lists","chapter":"5 Coding style","heading":"5.2.2 Lists and .lists","text":"Now data tidy place live operating system. Nice. data R?Everything R lives Global Environment. can read , free time specific questions.previously touched vectors lists, main difference vectors take one kind element lists can store whatever fancy. ’s want.Imagine need compute following incredibly complicated numbers want store meeting PI later:Omg, see Global Environment top right gets cluttered. PI won’t like . Let’s tidy using list.Click important_numbers top right19 can view content list. equivalent writing View(important_numbers) console R script.Finally, let’s clean mess deleting previously created R objects named , b, c . can rm() function. different ways use .One last trick. notice never saw .c Global Environment. created hidden object . prefix. can find objects(.names = T) remove just .good practice clean environment end analysis chunk end script remove things don’t need anymore. talking gigabytes data instead lousy numbers, try gc() deleting objects.20","code":"\na <- 1 + 1\nb <- 2 - 3\nc <- 2 * Inf\nd <- 1 / 0\ne <- 2^2\nf <- sqrt(2)\ng <- exp(1)\nh <- log10(3)\ni <- log2(42)\nj <- log(pi, base = pi)\nk <- a > b\nl <- a < b\nm <- a == b\nn <- a >= b\no <- a <= b\n# Directly store the first numbers in the list\nimportant_numbers <- list(\n  a = 1 + 1, \n  b = 2 - 3, \n  c = 2 * Inf, \n  # You can also use a string \"d\" to define the name, it's the same\n  \"d\" = 1 / 0)\n\n# Add the remaining numbers\nimportant_numbers[[\"e\"]] <- 2^2\nimportant_numbers[[\"f\"]] <- sqrt(2)\nimportant_numbers[[\"g\"]] <- exp(1)\nimportant_numbers[[\"h\"]] <- log10(3)\nimportant_numbers[[\"i\"]] <- log2(42)\nimportant_numbers[[\"j\"]] <- log(pi, base = pi)\nimportant_numbers[[\"k\"]] <- a > b\nimportant_numbers[[\"l\"]] <- a < b\nimportant_numbers[[\"m\"]] <- a == b\nimportant_numbers[[\"n\"]] <- a >= b\nimportant_numbers[[\"o\"]] <- a <= b\n# Remove one object\nrm(a)\n# Remove multiple object \nrm(b, c)\n# Or \nrm(list = c(\"d\", \"e\"))\n# To remove everything use the object() function\nobjects()\n# and combine the two\nrm(list = objects())\n# Uups, everthing is gone. If you want to keep something add setdiff\na <- 1\nb <- 2\n.c <- 3\n# Remove all but a\nrm(list = setdiff(objects(), \"a\"))"},{"path":"coding-style.html","id":"summary","chapter":"5 Coding style","heading":"5.3 Summary","text":"Thanks making far. intro R Basics ended quite extensive, getting give tools figure problems R . examples useful yet, may come handy later. Basically every piece code used work one point.lot functions get know, especially tidyverse, cover way. Please remember:Question everything understand seems unclear.always one way get goal, redundancy helps us understand things better.Let us know, different better way something, ’re discuss!","code":""},{"path":"our-project.html","id":"our-project","chapter":"6 Our project","heading":"6 Our project","text":"work mass spectrometry (MS) data obtained new proteomics method called PELSA (peptide-centric local stability assay enables proteome-scale identification protein targets binding regions diverse ligands). PELSA method combines short tryptic digestion proteins interest liquid chromatography-tandem MS (LC-MS/MS) analysis. differential conditions like drug treatment cells specific ligand like metabolites nucleic acids called limited proteolysis identifies sides proteins shielded digestion Trypsin stabilization protein-ligand complex (see Fig.1a).\n, Workflow PELSA. E/S ratio (wt/wt). b, Volcano plot visualization peptides PELSA analysis BT474 lysates exposed 100 nM lapatinib. c, Volcano plot b protein level. d, Local stability profiles reveal ligand-binding regions. upper lower boundaries gray-shaded area represent log2FCs 0.3 −0.3, respectively. x axis represents protein sequence N-terminal C-terminal. e, Local affinity profiles reveal local binding affinity ligand. Heatmap representation log2 peptide fold changes ERBB2 increasing lapatinib concentrations (0, 0.1, 1, 10 100 μM). f, Complex structure mTOR, rapamycin FKBP1A (PDB 1FAP). g, Volcano plot visualizations proteins PELSA analysis published LiP–MS analysis9 HeLa lysates exposed 2 μM rapamycin. h, Local stability profiles mTOR 2 μM rapamycin treatment. b, c g, P values, two-sided empirical Bayes t-test (four lysate replicates), adjustment. local stability plots, peptides |log2FC| > 0.3, passed significance cutoff (−log10P > 2, two-sided empirical Bayes t-test) retained ensure reliable differences peptide abundance. Schematics created using BioRender.com.\nLC-MS/MS measures abundance individual peptides can use quantitatively compare different conditions represented volcano plot Fig. 2a.\n, Volcano plot visualization proteins PELSA analyses K562 (left) HeLa (right) lysates exposed 20 μM staurosporine (four lysate replicates); P values, two-sided empirical Bayes t-test without adjustment. lower boundary red shadow denotes threshold −log10P, 80% stabilized proteins (log2FC < 0) protein kinases; threshold corresponds −log10P > 3.09 PELSA K562 dataset −log10P > 3.77 PELSA HeLa dataset. b, TPR evaluation selected assays staurosporine target identification. labeled points represent numbers identified candidate targets kinase targets assay (TPR 80%). LiP-Quant also labeled kinase target number 20 (TPR = 40%). gray line (slope 1) black dashed line (slope 0.8) represent 100 80% candidate targets kinase targets, respectively. c, Comparison protein sequence coverage LiP-Quant HeLa PELSA HeLa analyses whole quantified proteome (n = 5,601 versus n = 6,840 proteins) identified kinase targets (n = 24 versus n = 108 kinase targets). d, Fold changes kinase targets (n = 17) identified LiP-Quant (TPR cutoff 40%) PELSA (HeLa). c,d, P values, two-sided Wilcoxon rank sum test, adjustment; box plots show median (line, value labeled), upper lower quartiles (box) ±1.5× interquartile range (whiskers); outliers shown. e, Fold changes kinases quantified PELSA (K562), iTSA mTSA datasets. f, Melting temperatures identified kinase targets quantified kinases two PELSA datasets TPP dataset. PELSA kinase targets lack TPP-reported melting temperature values. g, Density plots showing −log10P distributions peptides tryptic cleavage sites located within outside kinase domains (KD) HeLa K562 PELSA analyses. dashed lines indicate −log10P cutoffs defined . doughnut charts show locations kinase peptides passed −log10P cutoffs. P values determined . b–f, Kinase targets refer protein kinases identified staurosporine-binding proteins; quantified kinases refer protein kinases dataset including protein kinases identified staurosporine-binding proteins. LiP-Quant, TPP, iTSA mTSA datasets retrieved literature4,9,17,19; LiP-Quant dataset acquired HeLa cell lysates, TPP, iTSA mTSA datasets obtained K562 cell lysates.Nowadays, proteomics publications accompanied raw MS data. Try find data publication Fig. 2a.\ndata can found following path :Data availabilityPXD034606PRIDE project URIPELSA_staurosporine_K562_peptides.csv\nGreat, data already looks quite clean. ’s work ‘rawer’ data course.","code":""},{"path":"working-with-data.html","id":"working-with-data","chapter":"7 Working with data","heading":"7 Working with data","text":"Finally, ’s happening. beginning work real data can apply new knowledge right away.","code":""},{"path":"working-with-data.html","id":"prepare-your-project","chapter":"7 Working with data","heading":"7.1 Prepare your project","text":"Let’s start opening fresh R project, making nice folder structure opening first script. kind script , R R Markdown work equally well, try find preference .Start project described save first script name like ‘R_group_project’ Scripts folder. Choose name like.\ncan choose kind script want clicking File -> New File -> R Script/R Markdown…/etc.save , press blue button top script says Save current document.\nfirst script ready code, let’s make nicer. suggest briefly describe script contains start loading first packages.Leave empty lines top, look professional. can also use # multiple ##s R scripts comments21 headings R Markdown script.","code":"\n# \n# R Group Project\n# Analysis of LC-MS/MS PELSA data to identify kinases \n# \n\n# Load libraries\nlibrary(tidyverse)"},{"path":"working-with-data.html","id":"import-your-data","chapter":"7 Working with data","heading":"7.2 Import your data","text":"now, link datasets Slack. , please complain. two files called report.tsv report.parquet. output files DIA-NN software, tool analyzes mass spectrometry data outputs table identified ions.22Download files place R project. , try figure functions need read files store objects data_raw.\nGoogle :)\nGoogle ‘import R tsv’, ‘import R parquet’.wasn’t , swear. . seriousness, many ways import data base R solutions, canonical tidyverse functions, cover different data types one, vroom. made life much, much easier automatically understands file format.functions read files begin read. read_. Type press Tab.Another file format like introduce .parquet, Apache Arrow accompanying Arrow R Package. one compatible vroom needs function.data tenth size therefore reads writes much faster. also convenient .csv files due clear format columns. ’m afraid doesn’t work Excel.\nfunctions also work URLs. Try :23\ncontinue data .parquet file, can delete object.","code":"\n# Simply this\ndata_raw <- vroom::vroom(\"Data/report.tsv\")\ndata_raw <- arrow::read_parquet(\"Data/report.parquet\")\ndata_raw <- vroom::vroom(\"https://www.owncloud.de/bioinfo/R/dataset.tsv\")"},{"path":"working-with-data.html","id":"understand-your-data","chapter":"7 Working with data","heading":"7.3 Understand your data","text":"Okay, got data R. now, already better Excel. Try open .tsv file Excel. need view . ?\nlists, either click object Global Environment top rigth type :\nGreat, first impression data scrolling columns. Since understanding data generate hard, can discuss together. want give try, DIA-NN basics importantly Main output reference. explains column names, look .idea looking , let’s see R provides us summary functions.summary functions use Intro R Monday?\nGoogle gave nice resource https://www.r-bloggers.com/2018/11/explore--dataset--r/.\ncouple examples get overview structure R data object.\nnecessary, simply View() data start look specific questions.","code":"\nView(data_raw, title = \"wow, we can add a title\")\n# Column names\nnames(data_raw)\n# or \ncolnames(data_raw)\n# Print the top of the data\nhead(data_raw)\n# Object structure\nstr(data_raw)\ndim(data_raw)\n# This my take some time with big data but gives you some statistics\nsummary(data_raw)"},{"path":"working-with-data.html","id":"data-format","chapter":"7 Working with data","heading":"7.3.1 Data format","text":"broad idea data, can start look things interested . scientific data follows structure .variables, case precursors (measured peptide ions). variable measured observations, samples. pair, values different data types. called wide data format. can read Tidy data section. Check applies data!\ncolumn Runs contains sample names observations example . addition, column Precursor.Id, variables. means working data wide format, long format.tidyr functions tidyr::pivot_wider() tidyr::pivot_longer() mentioned allow reshape data long wide back.","code":""},{"path":"working-with-data.html","id":"data-types","chapter":"7 Working with data","heading":"7.3.2 Data types","text":"start reshape data, need identify extract relevant data columns.Output data types per column combining two functions lapply typeof.?lapply ?typeof help understand individual functions. lapply useful function applying operation elements list vector. case, data_raw tibble/data frame can seen list columns. Therefore, specified function applied column.\npurrr::map function family replaces base R’s apply functions recommended.functions useful start working one dataset data frame .\ncan use opportunity include discussions Control flow code.Reframe problem Loop.Now idea columns data contain character data, mainly IDs, numeric data, quantitative values q-values.can combine identifying data types extracting columns combination dplyr::select() dplyr::(). 1. Select character columns. 2. Select numeric columns. 3. Combine useful ID columns numeric columns. helpful overview data well.\ncan find explanations functions online:\nselect \n() function accepts kind function can used test whether column selected .also use output previous map function indicate columns.\nnumeric data can found .numeric(). , since provide function object, want something immediately, leave () brackets.also provide basic use case select, can simply use column names “strings”. Pressing Tab cursor select function can helpful well.\nLet’s keep remaining time open explore dplyr functions. described Data transformation section listed Function reference useful ones :dplyr::select()dplyr::select()dplyr::filter()dplyr::filter()dplyr::arrange()dplyr::arrange()dplyr::pull()dplyr::pull()dplyr::bind_rows()dplyr::bind_rows()dplyr::mutate()dplyr::mutate()dplyr::summarise()dplyr::summarise()dplyr::rename()dplyr::rename()dplyr::full_join()dplyr::full_join()dplyr::inner_join()dplyr::inner_join()dplyr::left_join()dplyr::left_join()dplyr::right_join()dplyr::right_join()","code":"\n# The function FUN will be applied to each column of X\n# Notice that we did not add `()` brackets after `typeof`\nlapply(X = data_raw, FUN = typeof)\n# map works the same as lapply\nmap(data_raw, typeof)\n# Or \ndata_raw %>% \n  map(typeof)\ndata_raw %>% \n  select(where(is.character))\n# Create vector of character column names \nnames_character <- map_lgl(data_raw, is.character) %>% \n  which() %>% \n  names() \n# Use vector as input for select \ndata_raw %>% \n  select(names_character)\ndata_raw %>% \n  select(where(is.numeric))\ndata_raw %>% \n  # Combine the different ways of choosing columns in a vector c()\n  select(c(\"Precursor.Id\", where(is.numeric)))\n# We could also retain some more information\ndata_raw %>% \n  select(c(\"Precursor.Id\", \"Run\", \"Protein.Group\", \"Protein.Names\", \"Genes\"  , where(is.numeric)))\ndata_raw %>% \n  select(c(Precursor.Id, Modified.Sequence, \"Run\", \"Precursor.Charge\"))"},{"path":"working-with-data.html","id":"tidy-up-your-data","chapter":"7 Working with data","heading":"7.4 Tidy up your data","text":"list dplyr functions , pretty powerful tool set manipulate data ever way like. also need know, need , get data analysis-ready.Following , can start make data legible removing unnecessary information making names meaningful. follow scheme like :Filtering identified precursorsShorten clean sample namesRemove unnecessary information (select column need)Sum precursors modified peptidesPivot data samples x peptides matrix (still tibble tho)Count missing values filter data(Optional) Normalize data","code":""},{"path":"working-with-data.html","id":"filtering-identified-precursors","chapter":"7 Working with data","heading":"7.4.1 Filtering identified precursors","text":"Main output reference mentioned , end contains section Filtering main report. Choose filtering scheme think makes sense (match-runs (MBR) used).Choose parameters threshold filter precursors data assign new object data_filtered.\nDIA-NN documentation suggest, use threshold 0.01 Lib.Q.Value PG.Q.Value.Lib.Peptidoform.Q.Value also applied retroactively find outliers weirdly behaving data points.\ndplyr::filter() function allows us remove rows based logical operation results one TRUE/FALSE value per row.can condensed way.\nrelatively simple way filter data. many cases, parameters data-driven schemes (variance, etc.) fiter data.","code":"\ndata_filtered <- data_raw %>% \n  filter(Lib.Q.Value < 0.01) %>% \n  filter(PG.Q.Value < 0.01)\ndata_filtered <- data_raw %>% \n  filter(Lib.Q.Value < 0.01, PG.Q.Value < 0.01) "},{"path":"working-with-data.html","id":"shorten-and-clean-up-sample-names","chapter":"7 Working with data","heading":"7.4.2 Shorten and clean up sample names","text":"easier data can read understood, useful others. sometimes spend days, weeks months dataset. Therefore, familiar sample names, groups, etc. others understand meeting presentation, helpful names use concise meaningful possible. Let’s look sample names column Run. Currently, contain full names raw data files processed DIA-NN. Depending needed information, can remove redundant details sample names.Let’s check first:can see lot redundancy sample names, one describes whole experimental MS scheme. Shortening make data legible.Modify Run column data_filtered data frame shortening sample names useful parts.\nfigure modify names, makes easier extract names work test vector can implement code script.Useful functions text manipulation can found stringr package. get good overview functions package, Reference nicely lists everything need know. , functions, [base R equivalents](https://stringr.tidyverse.org/articles/-base.html. problem can start simple base R function substring.can go .\nmodifying data, either focus one column columns certain type. can accomplished dplyr::mutate(). function assigns new values column provided name, name existing column, overwrites .can see mutate function overwrites Run column defined left side = sign uses Run expression right side = sign. finish problem, still need adjust beginning desired substring.want count , copying use nchar() function.\nCombining hints, can reassign data_filtered object overwrite Run column.Make sure comfortable mutate function works!different way remove redundant part string stringr::str_remove() function.intendation code changed slightly, otherwise pattern argument reached outside 80 characters limit look nice. Synthax-wise, make difference code work just .View data frame see looks much nicer now.","code":"\n# The Run column can also be accessed with the dplyr::pull() function\ndata_filtered$Run %>% \n  # we only need each name once\n  unique()\nsample_names <- data_filtered %>% \n  pull(Run) %>% \n  unique()\nsample_names %>% \n  substring(first = 20)\ndata_filtered <- data_filtered %>% \n  mutate(Run = substring(text = Run, first = 20))\ndata_filtered <- data_filtered %>% \n  mutate(Run = substring(text = Run, first = 41))\ndata_filtered <- data_filtered %>% \n  mutate(Run = stringr::str_remove(\n    string = Run, \n    pattern = \"EVE_250123_S4504_LKJ_CP_PELSA_K562_Stau_\"))\nView(data_filtered)"},{"path":"working-with-data.html","id":"remove-unnecessary-information","chapter":"7 Working with data","heading":"7.4.3 Remove unnecessary information","text":"removing redundant stuff sample names, next step Tidy Data Campaign involves removing information need later. help us lot getting better overview data allow others look dataset without immediately overwhelmed.\n, Main output reference helpful Main report section Output chapter gives short summary. primarily interested information regarding samples, features precursors information like peptide sequence annotation well normalized quantity.Remove columns data_filtered need.\ndplyr::select() function allow us retain column need. Regarding columns need, just take guess ’ll find solution.also allowed us change order columns. two ways .dplyr::all_of() function check, specified column names can found data. dplyr::any_of() function , fail column name found. makes bit flexible, real advantage can use predifened vector list column names.Simply using vector works well, R complain don’t want . lead confusions code, real problem.\nGreat, data look much better already.24","code":"\ndata_filtered <- data_filtered %>% \n  select(c(\"Run\",\n           \"Precursor.Id\",\n           \"Modified.Sequence\",\n           \"Stripped.Sequence\",\n           \"Protein.Group\",\n           \"Protein.Names\",\n           \"Genes\",\n           \"Precursor.Normalised\"))\ndata_filtered <- data_filtered %>% \n  select(all_of(c(\"Run\",\n                  \"Precursor.Id\",\n                  \"Modified.Sequence\",\n                  \"Stripped.Sequence\",\n                  \"Protein.Group\",\n                  \"Protein.Names\",\n                  \"Genes\",\n                  \"Precursor.Normalised\")))\ncolumn_names <- c(\"Run\",\n                  \"Precursor.Id\",\n                  \"Modified.Sequence\",\n                  \"Stripped.Sequence\",\n                  \"Protein.Group\",\n                  \"Protein.Names\",\n                  \"Genes\",\n                  \"Precursor.Normalised\")\ndata_filtered <- data_filtered %>% \n  select(any_of(column_names))"},{"path":"working-with-data.html","id":"sum-precursors-to-modified-peptides","chapter":"7 Working with data","heading":"7.4.4 Sum precursors to modified peptides","text":"next step data-specific, allow us explore dplyr::summarise() function. data currently exists precursor level.25 peptide can multiple charge states based amino acid sequence pH buffer (e.g. 2+ 3+), can combine quantity represent truer peptide quantity. Simply summing values way go .Read dplyr::summarise() function sum Precursor.Normalised column precursors peptides create data_peptides. need identify column data identifies peptides. addition, check Precursor.Normalised contains NAs, sum() function sensitive .\nneed summarise precursors peptides identified Modified.Sequence individually sample/Run.\ndplyr::summarise() function works dplyr::mutate(), difference computation mutate yields output length input summarise reduces number elements output combining . used plain, output one row summary column. can specify sub groups combined .argument.\nfirst check Precursor.Normalised data contains NAs.Looks good, now can combine precursors.new data frame contains Run Modified.Sequence row IDs new Precursor.Normalised.sum column. want retain annotations peptides, can use function keep well. Since protein gene information peptide, can simply use unique reduce multiple entries one.\ncan condensed way.different informations precursor, use paste(..., collapse = \";\") c() keep information\nNow data operates ‘modified peptides’ level26, want differential abundance analysis .","code":"\ndata_filtered %>% \n  pull(Precursor.Normalised) %>% \n  is.na() %>% \n  # sum will give you the total number of NAs, table or mean work as well \n  sum()\ndata_peptides <- data_filtered %>% \n  # It can be useful to indicate the operations we do along the way as names \n  # Precursor.Normalised -> Precursor.Normalised.sum \n  summarise(Precursor.Normalised.sum = sum(Precursor.Normalised), \n            .by = c(\"Run\", \"Modified.Sequence\"))\ndata_peptides <- data_filtered %>% \n  summarise(Precursor.Normalised.sum = sum(Precursor.Normalised), \n            Stripped.Sequence = unique(Stripped.Sequence), \n            Protein.Group = unique(Protein.Group), \n            Protein.Names = unique(Protein.Names), \n            Genes = unique(Genes), \n            .by = c(\"Run\", \"Modified.Sequence\"))"},{"path":"working-with-data.html","id":"pivot-data-to-a-samples-x-peptides-matrix","chapter":"7 Working with data","heading":"7.4.5 Pivot data to a samples x peptides matrix","text":"data good way finally used Limma analysis. can check way pivoting wide format. allow us inspect data visually easier.came across tidyr::pivot_wider() function . Use spread data two ways: 1. data_peptides_o27 peptides become become column names 2. data_peptides_v28 sample names become column names.\ntidyr::pivot_wider() wider function three primary arguments dictate shape resulting data frame.\nfirst part exercise, want samples stay rows.Great, want! Now let’s opposite.Works well, lost data. can fix including column names id_cols.work retaining samples rows.","code":"\npivot_wider(id_cols = c(\"column that stays as row IDs\"), \n            names_from = c(\"column whose values will be used as column names\"), \n            values_from = c(\"column from which values are used for each cell\"))\ndata_peptides_o <- data_peptides %>% \n  pivot_wider(id_cols = \"Run\", \n              names_from = \"Modified.Sequence\", \n              values_from = \"Precursor.Normalised.sum\")\ndata_peptides_v <- data_peptides %>% \n  pivot_wider(id_cols = \"Modified.Sequence\", \n              names_from = \"Run\", \n              values_from = \"Precursor.Normalised.sum\")\ndata_peptides_v <- data_peptides %>% \n  pivot_wider(id_cols = c(\"Modified.Sequence\", \n                          \"Stripped.Sequence\",\n                          \"Protein.Group\",\n                          \"Protein.Names\",\n                          \"Genes\"), \n              names_from = \"Run\", \n              values_from = \"Precursor.Normalised.sum\")"},{"path":"working-with-data.html","id":"count-missing-values-and-further-filter-data","chapter":"7 Working with data","heading":"7.4.6 Count missing values and further filter data","text":"pretty nice data frames now. Let’s check start attack biggest enemy data science, missing values. many approaches handle missing values, simplest one remove data missing values. case, peptides missing values.29In principle, data frames data_peptides_o data_peptides_v used task. use data_peptides_v now. First need identify form missing values, can NAs 0s. Let’s count dplyr::summarise() function combination dplyr::across().Read can use dplyr::across() target multiple columns time 1. count NAs 2. 0s.dplyr::across() allows us specify column want modify similar dplyr::select() function dplyr::() function like .numeric(). second part dplyr::across() function needs function applied column separately. case, function count number NAs 0s.NAs regular values, including 0, sometimes behave different. need take account counting 0s data.first thing see yes, NAs data. numbers telling. can replace sum() mean() get fraction NAs data.counting number 0s data, intuitively .However, fail due NAs data. One quick dirty workaround temporarily set NAs 1.30Now first transformed NAs 0s, continued computation. addition mutate across, also used function ifelse() helpful, want different operations values depending value . , test value x NA .na(x) return 1 statement true value .31Besides , answer initial question, missing values form NAs.Now actually dealing data. already said want simply exclude peptides missing values. , counted number NAs per sample. Now, need peptide individually.Count number missing values per peptide/row using mutate dplyr::c_across() function add column f_values32 data_peptides_v. Assign new data frame data_peptides_v_c33.\nWasn’t description quite detailed. Anyway, dplyr::c_across() work similar dplyr::select() function. make sure, mutate row-wise.\nLet’s fist count number missing values per peptideWe can also visualize distribution.Inverting counting values looks actually better . can achive invering logical statement !.now filter table.Mutating filtering can also combined.finally compare number peptides data_peptides_v data_peptides_v_c see many peptides filtered .","code":"\ndata_peptides_v %>% \n  summarise(across(where(is.numeric), \\(x) sum(is.na(x))))\n# Optionally we can directly View the output of this computation\n# %>% View()\ndata_peptides_v %>% \n  summarise(across(where(is.numeric), \\(x) mean(is.na(x))))\ndata_peptides_v %>% \n  summarise(across(where(is.numeric), \\(x) mean(x == 0)))\ndata_peptides_v %>% \n  # First we replace all NAs by 1 with the combination of mutate and across\n  mutate(across(where(is.numeric), \\(x) ifelse(is.na(x), 1, x))) %>% \n  # then we can count the 0s without any trouble\n  summarise(across(where(is.numeric), \\(x) mean(x == 0)))\ndata_peptides_v %>% \n  mutate(f_values = mean(is.na(c_across(where(is.numeric)))), \n         .by = \"Modified.Sequence\", .after = 1) # .after and .before define the position of the new column\n# %>% View()\ndata_peptides_v %>% \n  mutate(f_values = mean(is.na(c_across(where(is.numeric)))), \n         .by = \"Modified.Sequence\", .after = 1) %>% \n  pull(f_values) %>% \n  table() %>% \n  barplot()\ndata_peptides_v %>% \n  # mean(is.na(... -> mean(!is.na(...\n  mutate(f_values = mean(!is.na(c_across(where(is.numeric)))), \n         .by = \"Modified.Sequence\", .after = 1) %>% \n  pull(f_values) %>% \n  table() %>% \n  barplot()\ndata_peptides_v %>% \n  mutate(f_values = mean(!is.na(c_across(where(is.numeric)))), \n         .by = \"Modified.Sequence\", .after = 1) %>% \n  filter(f_values == 1) # Here we could adjust the threshold \n# 1 means no NAs\ndata_peptides_v_c <- data_peptides_v %>% \n  filter(mean(!is.na(c_across(where(is.numeric)))) == 1, \n         .by = \"Modified.Sequence\")"},{"path":"working-with-data.html","id":"optional-normalize-the-data","chapter":"7 Working with data","heading":"7.4.7 (Optional) Normalize the data","text":"already working column named Precursor.Normalised, suggests data somehow normalized, sometimes renormalizing makes sense general enough cases normalization essential. general, want eliminate systematic factor samples. means dividing peptide values one sample ones another sample, median factor one. based assumption bulk data change conditions.use limma function limma::normalizeBetweenArrays() normalize data. try figure use , however, took quite time involves couple uncommon functions get data right shape. Basically limma::normalizeBetweenArrays() accepts matrix variables/peptides rows. let’s simply look code.failed, ’s ! forgot remove column additional information peptides. quick way .can find explantion normalization method documentation.can explore impact renormalizing data later calculating coefficient variance (CV) group PCA analysis data frames.data looks pretty good now ready Limma analysis identify peptides affected abundance drug treatment. Now can simply see can store data done working simply reopen sitting .","code":"\n# We assign a new object \ndata_norm <- data_peptides_v_c %>% \n  # Column_to_rownames allows us to make a matrix from our tibble \n  # and store the character column 'Modified.Sequence' as rownames\n  tibble::column_to_rownames(var = \"Modified.Sequence\") %>%\n  # Normalization with the scale-method = median-normalization\n  limma::normalizeBetweenArrays(method = \"scale\") %>%\n  # The following lines revert the matrix to a tibble\n  as.data.frame() %>%\n  tibble::rownames_to_column(var = \"Modified.Sequence\") %>%\n  tibble::as_tibble()\n# We assign a new object \ndata_norm <- data_peptides_v_c %>% \n  select(c(1, where(is.numeric))) %>% \n  tibble::column_to_rownames(var = \"Modified.Sequence\") %>%\n  limma::normalizeBetweenArrays(method = \"scale\") %>%\n  as.data.frame() %>%\n  tibble::rownames_to_column(var = \"Modified.Sequence\") %>%\n  tibble::as_tibble()"},{"path":"working-with-data.html","id":"save-your-data","chapter":"7 Working with data","heading":"7.5 Save your data","text":"raw data imported first data transformation done, clean environment break code separate script.need save R Environment keep data objects together. , can useful delete intermediate objects everything need anymore.Now, everything saved can conveniently accessed later loading data environment load(\"Data/RData/00_Import_data.RData\"). also nice way share data others.","code":"\n# Clean environment\nrm(list = c())\n\n# Save RData\nsave.image(\"Data/RData/00_Import_data.RData\")"},{"path":"limma-analysis.html","id":"limma-analysis","chapter":"8 Limma analysis","heading":"8 Limma analysis","text":", wrangling two days data, making pretty tidy without ever seeing heatmap , ready differential abundance analysis limma. limma tool originally developed microarray analysis, works just well proteomics data.34 use identify, peptides different abundance two sample groups.","code":""},{"path":"limma-analysis.html","id":"running-limma","chapter":"8 Limma analysis","heading":"8.1 Running limma","text":"can find limma package Bioconductor. Check website find 1. install , 2. limma User’s Guide 3. Reference Manual.35 Bioconductor package pages made way allow us find relevant information quickly. Except case.limma User’s Guide Reference Manual\nAdmittedly, helpful yet. packages, can find much concise documentation commonly one workflow descriptions depending want . guide seems bit practical can applied data.Let’s make real world example! Try apply limma analysis data. Use explanations guide page 36/37 limma User’s Guide. Help individual functions can found Reference Manual known ?limma::lmFit.final goal use limma::topTable() function extract results. fun good luck!\nsure want give already?\nGreat! Just continue trying something eventually work. worst case, use ChatG…, please don’t .\nOkay, go. Since analysis involves couple steps like clutter environment much, propose keep intermediate steps analysis one list. Don’t worry, result simply multiple R objects.first add data, already nice prepared. thing missing take log2 data, limma like statistical tests like t-test assume normally distributed data.good practise save input data close statistical tests.limma functions, similar limma::normalizeBetweenArrays(), accept matrix eSet (expression set), special type storing data.ExpressionSet idea standardize data formats R, practically used packages input people mainly work tibbles.Next, need describe experimental setup. can find information sample name names(limma[[\"data\"]]) describes either ‘Control’ ‘Staurosporine’ group, can simply make vector describes sample groups order samplesThe vector can made way. c(rep(\"Control\", 4), rep(\"Staurosporine\", 4)) just one many ways accomplish . rep(c(\"Control\", \"Staurosporine\"), = 4) work well fully writing names individually.names look weird, can rename generated model matrix. look see encode group information .Now can start first linear model fit limma::lmFit(). needs eset design matrix input.Following , need indicate comparisons want conduct. case two groups complex factorial design, can indicate interesting difference .Now compare groups.finally compute statistics.final results can extracted limma::topTable().Great, go first results. Let’s explore can work .","code":"\n# Install BiocManager if not yet installed\nif (!require(\"BiocManager\", quietly = TRUE))\n  install.packages(\"BiocManager\")\n# Install limma\nBiocManager::install(\"limma\")\nlimma_list <- list()\n## Add base data frame \nlimma_list[[\"data\"]] <- data_norm %>% \n  # This works simply with mutate(across(where(is.numeric), log2)) or with the \n  # fast version by pivoting in long format and back after the operation\n  tidyr::pivot_longer(-1) %>% \n  dplyr::mutate(value = log2(value)) %>% \n  tidyr::pivot_wider()\n## Add eset \nlimma_list[[\"eset\"]] <- limma_list[[\"data\"]] %>%\n  # We have seen this before, but here is the generalized example by renaming \n  # the first column to \"rowname\", which is the default column to use as \n  # rownames for column_to_rownames\n  dplyr::rename(rowname = 1) %>% \n  tibble::column_to_rownames() %>%\n  as.matrix() %>% \n  # Depending on your input data frame, you may need to transpose your data\n  # an eset has samples as columns and features/variables as rows\n  t() %>% \n  Biobase::ExpressionSet()\nlimma_list[[\"design\"]] <- \n  model.matrix(\n    ~0+factor(c(rep(\"Control\", 4), rep(\"Staurosporine\", 4)), \n              levels = c(\"Control\", \"Staurosporine\")))\ncolnames(limma_list[[\"design\"]]) <- c(\"Control\", \"Staurosporine\")\nlimma_list[[\"fit\"]] <- limma::lmFit(limma_list[[\"eset\"]], limma_list[[\"design\"]])\nlimma_list[[\"contrast.matrix\"]] <- limma::makeContrasts(Staurosporine-Control, \n                                                        levels=limma_list[[\"design\"]])\nlimma_list[[\"fit2\"]] <- limma::contrasts.fit(fit = limma_list[[\"fit\"]], \n                                             contrasts = limma_list[[\"contrast.matrix\"]])\nlimma_list[[\"fit2_eBayes\"]] <- limma::eBayes(limma_list[[\"fit2\"]])\nlimma::topTable(limma_list[[\"fit2_eBayes\"]]) # %>% View()"},{"path":"limma-analysis.html","id":"filtering-the-limma-output","chapter":"8 Limma analysis","heading":"8.2 Filtering the limma output","text":"limma::topTable() given preview results. recommend using different functions , biobroom::tidy.MArrayLM() function outputs nice tibble. biobroom Bioconductor function helps “[…] methods converting standard objects constructed bioinformatics packages, especially Bioconductor, converting tidy data”.Try look results!36\ncan View() results bring peptides lowest p-value top clicking p.value column. Still, still generic terms data frame like gene. can modify help legibility.Repeat last command modify data frame renaming gene column something like id arrange rows p.value.\nNice, now can look data closely. common practise applying statistical test many different measurements p-value correction. ","code":"\nresults_list[[\"results\"]] <- \n  biobroom::tidy.MArrayLM(results_list[[\"fit2_eBayes\"]])\nresults_list[[\"results\"]] <- biobroom::tidy.MArrayLM(results_list[[\"fit2_eBayes\"]]) %>% \n  dplyr::rename(id = gene) %>% \n  dplyr::arrange(p.value)\n# Extract results \nrequire(biobroom)\nresults_list[[\"results\"]] <- biobroom::tidy.MArrayLM(results_list[[\"fit2_eBayes\"]]) %>% \n  # rename contrasts \n  dplyr::mutate(term = paste(rev(conditions), collapse = \" - \")) %>% \n  dplyr::rename(id = gene) %>% \n  dplyr::arrange(p.value) %>% \n  # Adjust p-values\n  dplyr::mutate(p.adjust = \n                  p.adjust(p.value, \n                           method = results_list[[\"par\"]]$p.adjust.method), \n                .after = \"p.value\") %>% \n  # Apply thresholds\n  dplyr::mutate(regulation = dplyr::case_when(\n    p.adjust < results_list[[\"par\"]]$p.threshold & \n      estimate >= results_list[[\"par\"]]$fc.threshold ~ \"up\", \n    p.adjust < results_list[[\"par\"]]$p.threshold & \n      estimate <= - results_list[[\"par\"]]$fc.threshold ~ \"down\", \n    .default = \"none\"\n  ))"},{"path":"limma-analysis.html","id":"visualizing-the-limma-results","chapter":"8 Limma analysis","heading":"8.3 Visualizing the limma results","text":"","code":""},{"path":"limma-analysis.html","id":"summary-1","chapter":"8 Limma analysis","heading":"8.4 Summary","text":"","code":""},{"path":"reporting-results.html","id":"reporting-results","chapter":"9 Reporting results","heading":"9 Reporting results","text":"Coming Thursday.\n","code":""},{"path":"more-data-exploration.html","id":"more-data-exploration","chapter":"10 More data exploration","heading":"10 More data exploration","text":"’ll design chapter together.","code":""},{"path":"version-control-with-github.html","id":"version-control-with-github","chapter":"12 Version control with GitHub","heading":"12 Version control with GitHub","text":"Excuse , moment talk version control?37There lot say GitHub one may use . extensive discussion topic basically everything learn chapter can found Happy Git GitHub useR.chapter introduce basics collaborate people using GitHub, probably reason reading first place.","code":""},{"path":"version-control-with-github.html","id":"headstart-into-git-and-github-with-rstudio","chapter":"12 Version control with GitHub","heading":"12.1 Headstart into Git and GitHub with RStudio","text":"following post provides quick introduction set Git GitHub connect GitHub account RStudio: https://www.bioinformatics.babraham.ac.uk/training/RStudio_GitHub/Initial_setup.html.done, able connect download online GitHub repositories able start collaborating projects immediately.","code":""},{"path":"version-control-with-github.html","id":"basic-github-routine","chapter":"12 Version control with GitHub","heading":"12.2 Basic GitHub routine","text":"Open Git terminal R start lines code.Add files:Commit changes:Push commits:…","code":"git add .git commit -m \"Add important changes\"git push"},{"path":"version-control-with-github.html","id":"common-problems","chapter":"12 Version control with GitHub","heading":"12.3 Common problems","text":"following provide summary common problems encountered using Git. also serves reminder .","code":""},{"path":"version-control-with-github.html","id":"too-large-files","chapter":"12 Version control with GitHub","heading":"12.3.1 Too large files","text":"Original post answer: https://stackoverflow.com//17890278.Download BFG Repo-Cleaner jar file “bfg-x.xx.x.jar” (e.g. “bfg-1.14.0.jar”) https://rtyley.github.io/bfg-repo-cleaner/.Download BFG Repo-Cleaner jar file “bfg-x.xx.x.jar” (e.g. “bfg-1.14.0.jar”) https://rtyley.github.io/bfg-repo-cleaner/.Place file directory R project, .git folder.Place file directory R project, .git folder.Open terminal folder (e.g. via RStudio > Git > Shell…)Open terminal folder (e.g. via RStudio > Git > Shell…)Type terminal:Type terminal:file name “bfg.jar” must match name jar file file size limit can changed (e.g. 50M 50 )encouter error, type:clean dead data.encounter following error Warning : large blobs matching criteria found packfiles - repo need packed?, refer post https://stackoverflow.com/q/61769785 type git gc prior step 4.","code":"java -jar bfg.jar --strip-blobs-bigger-than 100Mgit gc --prune=now --aggressive"},{"path":"version-control-with-github.html","id":"gitignore-does-not-instantly-work","chapter":"12 Version control with GitHub","heading":"12.3.2 .gitignore does not instantly work","text":"Just :","code":"git rm -r --cached .\ngit add .\ngit commit -m \"Drop files from .gitignore\""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
